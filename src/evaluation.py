import os
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from dotenv import load_dotenv
from tqdm import tqdm  # ì§„í–‰ë¥  í‘œì‹œë°”

# ê¸°ì¡´ ëª¨ë“ˆ ì„í¬íŠ¸
from src.vector_db import RFPVectorDB
from src.generator import RFPGenerator

load_dotenv()

# ==========================================
# 1. í‰ê°€ìš© ë°ì´í„°ì…‹ (Ground Truth) ì¤€ë¹„
# ì‹¤ì œ ë°ì´í„°ì— ë§ì¶° ì§ˆë¬¸ê³¼ ì •ë‹µì„ ëŠ˜ë ¤ë‚˜ê°€ì„¸ìš”.
# ==========================================
TEST_DATASET = [
    {
        "question": "í•œì˜ëŒ€í•™êµ í•™ì‚¬ì •ë³´ì‹œìŠ¤í…œ ê³ ë„í™” ì‚¬ì—…ì˜ ì˜ˆì‚°ì€ ì–¼ë§ˆì¸ê°€?",
        "ground_truth": "130,000,000ì› (1ì–µ 3ì²œë§Œ ì›)"
    },
    {
        "question": "ë¶€ì‚°êµ­ì œì˜í™”ì œ ì˜¨ë¼ì¸ì„œë¹„ìŠ¤ ì¬ê°œë°œ ì‚¬ì—…ì˜ ë°œì£¼ ê¸°ê´€ì€ ì–´ë””ì¸ê°€?",
        "ground_truth": "(ì‚¬)ë¶€ì‚°êµ­ì œì˜í™”ì œ"
    },
    {
        "question": "ì´ í”„ë¡œì íŠ¸ì—ì„œ ë‹¤ë£¨ëŠ” ë¬¸ì„œì˜ ì¢…ë¥˜ëŠ” ë¬´ì—‡ì¸ê°€?",
        "ground_truth": "ì œì•ˆìš”ì²­ì„œ(RFP)"
    }
]


class RFPEvaluator:
    def __init__(self):
        # ì±„ì ê´€ ëª¨ë¸ (Judge)
        self.judge_llm = ChatOpenAI(model="gpt-5-mini", temperature=0)

        # ì‹œìŠ¤í…œ ëª¨ë“ˆ ë¡œë“œ
        self.db_manager = RFPVectorDB(db_path="./chroma_db")
        self.retriever = self.db_manager.get_retriever()
        self.generator = RFPGenerator()

    def evaluate(self):
        print(f"ğŸ“Š ì´ {len(TEST_DATASET)}ê°œì˜ ë¬¸í•­ì— ëŒ€í•´ í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

        score = 0
        results = []

        for item in tqdm(TEST_DATASET):
            question = item['question']
            truth = item['ground_truth']

            # 1. ìš°ë¦¬ AIì˜ ë‹µë³€ ìƒì„±
            relevant_docs = self.retriever.invoke(question)
            # í‰ê°€ëŠ” ë‹¨ë°œì„± ì§ˆë¬¸ì´ë¯€ë¡œ chat_historyëŠ” ë¹„ì›Œë‘¡ë‹ˆë‹¤.
            prediction = self.generator.generate_answer(question, relevant_docs, chat_history=[])

            # 2. LLM ì±„ì  (Judge)
            is_correct = self.judge_answer(question, truth, prediction)

            if is_correct:
                score += 1
                results.append("âœ… ì •ë‹µ")
            else:
                results.append("âŒ ì˜¤ë‹µ")

            # ë””ë²„ê¹…ìš© ì¶œë ¥ (í•„ìš” ì‹œ ì£¼ì„ í•´ì œ)
            # print(f"\nQ: {question}")
            # print(f"A(AI): {prediction}")
            # print(f"A(Truth): {truth}")
            # print(f"Result: {'Pass' if is_correct else 'Fail'}")

        # ìµœì¢… ë¦¬í¬íŠ¸
        accuracy = (score / len(TEST_DATASET)) * 100
        print("\n" + "=" * 30)
        print("      ğŸ† í‰ê°€ ê²°ê³¼ ë¦¬í¬íŠ¸      ")
        print("=" * 30)
        print(f"ì´ ë¬¸í•­ ìˆ˜ : {len(TEST_DATASET)}")
        print(f"ì •ë‹µ ìˆ˜   : {score}")
        print(f"ì˜¤ë‹µ ìˆ˜   : {len(TEST_DATASET) - score}")
        print(f"ìµœì¢… ì •í™•ë„ : {accuracy:.2f}%")
        print("=" * 30)

    def judge_answer(self, question, truth, prediction):
        """
        AI ë‹µë³€ì´ ì •ë‹µê³¼ ì˜ë¯¸ì ìœ¼ë¡œ ì¼ì¹˜í•˜ëŠ”ì§€ LLMì—ê²Œ ë¬¼ì–´ë´…ë‹ˆë‹¤.
        """
        judge_prompt = ChatPromptTemplate.from_messages([
            ("system", "ë‹¹ì‹ ì€ ê³µì •í•œ ì±„ì ê´€ì…ë‹ˆë‹¤. [AI ë‹µë³€]ì´ [ì •ë‹µ]ì˜ í•µì‹¬ ë‚´ìš©ì„ ì •í™•íˆ í¬í•¨í•˜ê³  ìˆëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”. "
                       "í˜•ì‹ì´ ë‹¬ë¼ë„ í•µì‹¬ ì •ë³´(ìˆ«ì, ê¸°ê´€ëª… ë“±)ê°€ ë§ìœ¼ë©´ ì •ë‹µì…ë‹ˆë‹¤. "
                       "ì •ë‹µì´ë©´ 'YES', ì˜¤ë‹µì´ë©´ 'NO'ë¼ê³ ë§Œ ëŒ€ë‹µí•˜ì„¸ìš”."),
            ("human", "ì§ˆë¬¸: {question}\nì •ë‹µ: {truth}\nAI ë‹µë³€: {prediction}")
        ])

        chain = judge_prompt | self.judge_llm | StrOutputParser()
        result = chain.invoke({
            "question": question,
            "truth": truth,
            "prediction": prediction
        })

        return "YES" in result.upper()


if __name__ == "__main__":
    evaluator = RFPEvaluator()
    evaluator.evaluate()