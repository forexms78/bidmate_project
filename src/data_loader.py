# src/data_loader.py
# @title ìˆ˜ì •ëœ RFPDataLoader ì „ì²´ ì½”ë“œ
import os
import pandas as pd
from typing import List
from tqdm import tqdm
from langchain_core.documents import Document
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

class RFPDataLoader:
    def __init__(self, file_path: str):
        self.csv_path = file_path
        self.base_dir = os.path.dirname(file_path)
        self.files_dir = os.path.join(self.base_dir, "files")

        # ìš”ì•½ì„ ìœ„í•œ LLM (ë¹ ë¥´ê³  ì €ë ´í•œ gpt-4o-mini ì‚¬ìš© ê°€ì •)
        # ëª¨ë¸ëª…ì€ ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ëª…ìœ¼ë¡œ í™•ì¸í•´ì£¼ì„¸ìš” (ì˜ˆ: gpt-4o-mini, gpt-3.5-turbo ë“±)
        self.summary_llm = ChatOpenAI(model="gpt-5", temperature=0)

    def summarize_content(self, text: str, meta: dict) -> str:
        """
        ê¸´ í…ìŠ¤íŠ¸ë¥¼ RAGì— ë„£ê¸° ì¢‹ê²Œ í•µì‹¬ë§Œ ìš”ì•½í•©ë‹ˆë‹¤.
        """
        if not text or len(text) < 100:  # ë„ˆë¬´ ì§§ìœ¼ë©´ ìš”ì•½ ì•ˆ í•¨
            return text

        template = """
        ë‹¹ì‹ ì€ ê³µê³µ ì…ì°° ë¬¸ì„œ ì „ì²˜ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
        ì•„ë˜ ë¬¸ì„œë¥¼ RAG ê²€ìƒ‰ì— ìµœì í™”ë˜ë„ë¡ í•µì‹¬ë§Œ ìš”ì•½í•˜ì„¸ìš”.

        [í•„ìˆ˜ í¬í•¨ ì •ë³´]
        1. ë¬¸ì„œ íŒŒì¼ í˜•ì‹: ì›ë³¸ íŒŒì¼ì˜ í™•ì¥ìê°€ {ext}ì„ì„ ë°˜ë“œì‹œ ëª…ì‹œ (ì˜ˆ: "ì´ ë¬¸ì„œëŠ” hwp íŒŒì¼ì…ë‹ˆë‹¤.")
        2. ì‚¬ì—…ëª…: {title}
        3. ì˜ˆì‚° ì •ë³´: ê¸ˆì•¡ ê´€ë ¨ ë‚´ìš©ì´ ìˆë‹¤ë©´ ìˆ«ìì™€ ë‹¨ìœ„(ì›)ë¥¼ ì •í™•íˆ ëª…ì‹œ
        4. ë°œì£¼ ê¸°ê´€: {agency}
        5. í•µì‹¬ ìš”ì•½: ì‚¬ì—…ì˜ ëª©ì ê³¼ ì£¼ìš” ê³¼ì—… ë‚´ìš©ì„ 3ì¤„ ë‚´ì™¸ë¡œ ìš”ì•½

        [ì›ë³¸ í…ìŠ¤íŠ¸ ì¼ë¶€]
        {text}

        [ìš”ì•½ ê²°ê³¼]
        """
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | self.summary_llm | StrOutputParser()

        try:
            # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì•ë¶€ë¶„ 4000ìë§Œ ì˜ë¼ì„œ ìš”ì•½ (í† í° ë¹„ìš© ì ˆì•½)
            return chain.invoke({
                "text": text[:4000],
                "title": meta['title'],
                "agency": meta['agency'],
                "ext": meta['file_ext'] # ë©”íƒ€ë°ì´í„° í‚¤ ë³€ê²½ ë°˜ì˜
            })
        except Exception as e:
            print(f"âš ï¸ ìš”ì•½ ì¤‘ ì—ëŸ¬ ë°œìƒ (ê±´ë„ˆëœ€): {e}")
            return text[:500] + "..."  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì•ë¶€ë¶„ë§Œ ë°˜í™˜

    def load(self, use_summary: bool = False) -> List[Document]:
        """
        :param use_summary: Trueë©´ LLMì„ í†µí•´ ë‚´ìš©ì„ ìš”ì•½ í›„ ì €ì¥í•©ë‹ˆë‹¤.
        """
        if not os.path.exists(self.csv_path):
            raise FileNotFoundError(f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.csv_path}")

        try:
            df = pd.read_csv(self.csv_path, encoding='utf-8')
        except UnicodeDecodeError:
            df = pd.read_csv(self.csv_path, encoding='cp949')

        all_docs = []
        print(f"ğŸ“Š ì´ {len(df)}ê°œì˜ ë°ì´í„° ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤... (ìš”ì•½ ëª¨ë“œ: {'ON' if use_summary else 'OFF'})")

        for idx, row in tqdm(df.iterrows(), total=len(df), desc="ë¬¸ì„œ ë¡œë”© ì¤‘"):
            file_name = str(row.get('íŒŒì¼ëª…', ''))
            file_path = os.path.join(self.files_dir, file_name)

            # [ìˆ˜ì •ë¨] 1. í™•ì¥ì ì¶”ì¶œ ë¡œì§ ê°œì„  (os.path ì‚¬ìš©ìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ)
            _, ext_temp = os.path.splitext(file_name)
            # ì (.)ì„ ì œê±°í•˜ê³  ì†Œë¬¸ìë¡œ ë³€í™˜ (ì˜ˆ: .HWP -> hwp)
            clean_ext = ext_temp.lower().replace('.', '') if ext_temp else 'ì•Œìˆ˜ì—†ìŒ'

            # 2. í…ìŠ¤íŠ¸ ì¶”ì¶œ (clean_ext ì‚¬ìš©)
            content = ""
            # PDF ì²˜ë¦¬
            if clean_ext == 'pdf' and os.path.exists(file_path):
                try:
                    loader = PyPDFLoader(file_path)
                    pages = loader.load()
                    content = "\n".join([p.page_content for p in pages])
                except Exception:
                    content = row.get('í…ìŠ¤íŠ¸', '')
            # DOCX ì²˜ë¦¬
            elif clean_ext in ['docx', 'doc'] and os.path.exists(file_path):
                try:
                    loader = Docx2txtLoader(file_path)
                    pages = loader.load()
                    content = "\n".join([p.page_content for p in pages])
                except Exception:
                    content = row.get('í…ìŠ¤íŠ¸', '')
            # HWP ë˜ëŠ” ê¸°íƒ€ (CSV í…ìŠ¤íŠ¸ ì‚¬ìš©)
            else:
                content = row.get('í…ìŠ¤íŠ¸', '')

            # ë‚´ìš© ì—†ìœ¼ë©´ ìŠ¤í‚µ
            if pd.isna(content) or str(content).strip() == "":
                continue

            # [ìˆ˜ì •ë¨] 3. ë©”íƒ€ë°ì´í„° ìƒì„± (file_ext ì¶”ê°€)
            metadata = {
                "source": file_name,
                "title": row.get('ì‚¬ì—…ëª…', 'ë¬´ì œ'),
                "agency": row.get('ë°œì£¼ ê¸°ê´€', 'ì•Œìˆ˜ì—†ìŒ'),
                "budget": row.get('ì‚¬ì—… ê¸ˆì•¡', 0),
                "file_ext": clean_ext, # â˜… í•µì‹¬: í•„í„°ë§ì„ ìœ„í•œ ì •í™•í•œ í™•ì¥ì í‚¤
                "extension": clean_ext # (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€)
            }

            # 4. ìš”ì•½ ëª¨ë“œ ì ìš© ì—¬ë¶€
            final_content = ""
            if use_summary:
                summary = self.summarize_content(content, metadata)
                final_content = f"[[AI ìš”ì•½ ì •ë³´]]\n{summary}\n\n================\n[[ì›ë³¸ ìƒì„¸ ë‚´ìš©]]\n{content}"
            else:
                final_content = (
                    f"[[ë¬¸ì„œ ì •ë³´]]\n"
                    f"- íŒŒì¼í˜•ì‹: {clean_ext}\n" # clean_ext ì‚¬ìš©
                    f"- ì‚¬ì—…ëª…: {metadata['title']}\n"
                    f"- ê¸°ê´€: {metadata['agency']}\n"
                    f"- ì˜ˆì‚°: {metadata['budget']}\n"
                    f"================\n{content}"
                )

            doc = Document(page_content=final_content, metadata=metadata)
            all_docs.append(doc)

        print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ! ì´ {len(all_docs)}ê°œ ë¬¸ì„œ.")
        return all_docs